{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "##Loading All the libraries\n",
    "import os\n",
    "import spacy\n",
    "import numpy as np\n",
    "import requests\n",
    "from nltk import pos_tag\n",
    "# coding: utf-8\n",
    "from spacy import displacy\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from requests import get\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bs4 import BeautifulSoup\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "be626263bc2140dee2c43dc5f35437f3c2a0f54c"
   },
   "outputs": [],
   "source": [
    "#loading Urls\n",
    "url1 = 'https://csee.essex.ac.uk/staff/udo/index.html'\n",
    "url2= 'https://www.essex.ac.uk/departments/computer-science-and-electronic-engineering'\n",
    "url =[url1 , url2]\n",
    "docid = 0\n",
    "document = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "d4c50970a5efff950bb03c5c04f4bd1b68daa292"
   },
   "outputs": [],
   "source": [
    "#Function for Formating html parsing and returning text\n",
    "def parsing(url):\n",
    "    res = requests.get(url)\n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "    for script in soup([\"script\", \"style\", 'aside']):\n",
    "        script.extract()\n",
    "    text = \" \".join(re.split(r'[\\n\\t]+', soup.get_text()))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "7458489be111d1d6ed9b4bcb74655f47460b5664"
   },
   "outputs": [],
   "source": [
    "#html parsing\n",
    "text =[]\n",
    "for i in url:\n",
    "    text.append(parsing(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "98a3b1437e73c43c65d66d146ffd94521bd677cb"
   },
   "outputs": [],
   "source": [
    "#Saving output to a file\n",
    "with open('Htmlprasing1.txt', 'w') as f:\n",
    "    print(text[0], file=f)\n",
    "with open('Htmlprasing2.txt', 'w') as f:\n",
    "    print(text[1], file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "3ff2a18b4bf57c39feec0734e7671d746f6d1de5"
   },
   "outputs": [],
   "source": [
    "#Function for Pre-processing-Removing stop words and lemmatization\n",
    "def text_processing(text):\n",
    "    text = text.lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    filtered_words = ' '.join(map(str, filtered_words))\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "0db7acbc0c0ae78cc1c31dad909f29a502491acd"
   },
   "outputs": [],
   "source": [
    "#Text pre-processing\n",
    "preprocessed_data1 = text_processing(text[0])\n",
    "preprocessed_data2 = text_processing(text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "bfb42dc0f0568424958891b6b71a4379312c1c80"
   },
   "outputs": [],
   "source": [
    "#Saving output to a file\n",
    "with open('preprocessed_data1.txt', 'w') as f:\n",
    "    print(preprocessed_data1, file=f)\n",
    "with open('preprocessed_data2.txt', 'w') as f:\n",
    "    print(preprocessed_data2, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "df104f5dc4c1345a86ce948fe6dc12da68edeb51"
   },
   "outputs": [],
   "source": [
    "#Function for word tokenization \n",
    "def tokenize(text):\n",
    "    words = word_tokenize(text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "b2a9122c1e9e3a3606dac067e3678302e7fcecbc"
   },
   "outputs": [],
   "source": [
    "#word tokenization\n",
    "wordtokens =[]\n",
    "for i in text:\n",
    "    wordtokens.append(tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "cb9beb1591150a3fa763b12b03a6583e4b3be64e"
   },
   "outputs": [],
   "source": [
    "#Saving output to a file\n",
    "with open('word_tokens1.txt', 'w') as f:\n",
    "    print(wordtokens[0], file=f)\n",
    "with open('word_tokens2.txt', 'w') as f:\n",
    "    print(wordtokens[1], file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "e9df337c803ee0990887a9c385ec213a971acb13"
   },
   "outputs": [],
   "source": [
    "#Function for Sentence tokenizing\n",
    "def senttokenize(text):\n",
    "    words = sent_tokenize(text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "549908d2d466ebab0f57c88259ec493420abf2aa"
   },
   "outputs": [],
   "source": [
    "#Sentence tokenization\n",
    "sentencetokens =[]\n",
    "for i in text:\n",
    "    sentencetokens.append(senttokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "d3ce425fbc859b7df075a8bc368fcb57d85407bb"
   },
   "outputs": [],
   "source": [
    "#Saving output to a file\n",
    "with open('sent_tokenize1.txt', 'w') as f:\n",
    "    print(sentencetokens[0], file=f)\n",
    "with open('sent_tokenize2.txt', 'w') as f:\n",
    "    print(sentencetokens[1], file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "3dd472046494e4d96766565cadf02aa219efb22b"
   },
   "outputs": [],
   "source": [
    "#Function for Part Of Speech tagging\n",
    "def pos_tagging(text):\n",
    "    words = word_tokenize(text)\n",
    "    return pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "ba2cb7b6a245bad69c6d40b2dfff569d3734ba3e"
   },
   "outputs": [],
   "source": [
    "#Part Of Speech tagging\n",
    "postags =[]\n",
    "for i in text:\n",
    "    postags.append(pos_tagging(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "0625f428559b4638c9af089df402dd3d7fedff54"
   },
   "outputs": [],
   "source": [
    "#Saving output to a file\n",
    "with open('POS_tagging1.txt', 'w') as f:\n",
    "    print(postags[0], file=f)\n",
    "with open('POS_tagging2.txt', 'w') as f:\n",
    "    print(postags[1], file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "4fb383fc2ee8b1c4ec0d7ff5d54ddec8022f7cc0"
   },
   "outputs": [],
   "source": [
    "#Function for Pre-processing-Removing stop words and lemmatization\n",
    "def text_processing(text):\n",
    "    text = text.lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    filtered_words = ' '.join(map(str, filtered_words))\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "b7e0450e09a8fdf653f19d77b002899a9a0928eb"
   },
   "outputs": [],
   "source": [
    "#Text pre-processing\n",
    "preprocessed_data1 = text_processing(text[0])\n",
    "preprocessed_data2 = text_processing(text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "6eb0ee1d8d0106825f42c8f3f3193fde32d9c0ec"
   },
   "outputs": [],
   "source": [
    "#Saving output to a file\n",
    "with open('preprocessed_data1.txt', 'w') as f:\n",
    "    print(preprocessed_data1, file=f)\n",
    "with open('preprocessed_data2.txt', 'w') as f:\n",
    "    print(preprocessed_data2, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "878ad479629f4006cdd6aa7e38100f99eade9e70"
   },
   "outputs": [],
   "source": [
    "#Creating corpus\n",
    "corpus=[preprocessed_data1,preprocessed_data2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "0b6888498898648ec5f456e284c72300eadc6329"
   },
   "outputs": [],
   "source": [
    "#Saving output to a file\n",
    "with open('corpus.txt', 'w') as f:\n",
    "    print(corpus, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "11aa41520c91103b3c81553cbf41cbbeb9cca092"
   },
   "outputs": [],
   "source": [
    "#Indexing\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "dd40ca4a1f0552a584bf444958d5ef96d51c0f6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in doc 1\n",
      "['committee' 'programme' 'co' 'workshop' 'ecir' '2015' '2014' '2018'\n",
      " '2016' 'organiser' '2013' 'project' 'information' 'sigir' 'conference'\n",
      " '2012' 'chair' '2017' 'paper' 'retrieval' 'demo' '2019' 'dr' '2010' 'lac'\n",
      " 'solution' '2011' 'day' 'track' 'web' 'search' 'acm' 'language' 'short'\n",
      " 'group' 'ktp' 'fdia' 'student' 'intelligence' 'ir' '2008' 'lrec' '2009'\n",
      " 'acl' 'signal' 'university' 'research' 'industry' 'data' 'udo']\n",
      "\n",
      "\n",
      "Top words in doc 2\n",
      "['research' 'essex' 'student' 'course' 'study' 'facility' 'campus'\n",
      " 'undergraduate' 'computer' 'master' 'postgraduate' 'excellence' 'contact'\n",
      " 'find' 'engineering' 'electronic' 'science' 'business' 'school'\n",
      " 'department' 'people' 'life' 'creative' 'degree' 'staff' 'specialist'\n",
      " 'colchester' 'use' '2019' 'university' 'suggestion' 'site' 'choose'\n",
      " 'opportunity' 'cookie' 'browse' 'imagination' 'cpd' 'unable' 'policy'\n",
      " 'lab' 'wise' 'uk' 'area' 'career' 'subject' 'centre' 'short' 'search'\n",
      " 'query']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Top 50 words\n",
    "n = 50\n",
    "for i in range(0,2):\n",
    "    feature_array = np.array(vectorizer.get_feature_names())\n",
    "    tfidf_sorting = np.argsort(X[i].toarray()).flatten()[::-1]\n",
    "    top_n = feature_array[tfidf_sorting][:n]\n",
    "    print(\"Top words in doc {}\".format(i+1))\n",
    "    print(top_n)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "f7ab2536d7472c67fbc8ccdcdf1cddda808fdbe1"
   },
   "outputs": [],
   "source": [
    "#Saving output to a file\n",
    "with open('top_50words.txt', 'w') as f:\n",
    "    print(top_n, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "9b7d7ac52a82b5e51146f1e83edbe68866fc80f8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Calculating feature scores\n",
    "Feature_scores=dict(zip(vectorizer.get_feature_names(),X.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "f54cd85f9142cf441e63ecf2b1bbfce2650c3373"
   },
   "outputs": [],
   "source": [
    "#Saving output to a file\n",
    "with open('Feature_scores.txt', 'w') as f:\n",
    "    print(Feature_scores, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "3bd9a72cb742f52a48349cd1901aa0d08da09187"
   },
   "outputs": [],
   "source": [
    "#Function for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "7184045b8db5aeb7a6e4cb8705b625a400c9f3bc"
   },
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "6e1664bcb013d99a4b38a80c8c47b5707c913cc1"
   },
   "outputs": [],
   "source": [
    "document1 = nlp(text[0])\n",
    "document2 = nlp(text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "633412ff937621fe438089b313e65008ff2954c9"
   },
   "outputs": [],
   "source": [
    "entity_listdoc1 = []\n",
    "\n",
    "for ent in document1.ents:\n",
    "    entity_listdoc1.append([ent, ent.label_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "96921c5758c08c8f8686bde8551d535593a2721b"
   },
   "outputs": [],
   "source": [
    "entity_listdoc2 = []\n",
    "\n",
    "for ent in document2.ents:\n",
    "    entity_listdoc2.append([ent, ent.label_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "ac7878d142d6438a19d63812e12042811560b14a"
   },
   "outputs": [],
   "source": [
    "#Saving output to a file\n",
    "with open('NER1.txt', 'w') as f:\n",
    "    print(entity_listdoc1, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "a00537a596bf77b1dc08ab0c5f35eac808e47e42"
   },
   "outputs": [],
   "source": [
    "#Saving output to a file\n",
    "with open('NER2.txt', 'w') as f:\n",
    "    print(entity_listdoc2, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "4f81a16a4a254e94f74b695935b5d455b3a838d6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
